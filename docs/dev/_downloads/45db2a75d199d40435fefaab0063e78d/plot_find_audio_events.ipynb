{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Use Audio to Align Video Data\nIn this example, we use ``pd-parser`` to find audio events using the same\nalgorithm for matching with time-stamps and rejecting misaligned\naudio, but applied using the onset of an audio deflection instead of detecting\nphotodiode events based on their square wave shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Alex Rockhill <aprockhill@mailbox.org>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load in a video with audio\n\nIn this example, we'll use audio and instead of aligning electrophysiology\ndata, we'll align a video. This example data is from a task where movements\nare played on a monitor for the participant to mirror and the video recording\nis synchronized by playing a pre-recorded clap. This clap sound, or a similar\nsound, is recommended for synchronizing audio because the onset is clear and\nallows good precision in synchronizing events.\n\nNote that the commands that require ffmpeg are pre-computed and commented\nout because ffmpeg must be installed to use them and it is not required by\n``pd-parser``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport os.path as op\nimport numpy as np\nfrom scipy.io import wavfile\nfrom subprocess import call\n# from subprocess import run, PIPE, STDOUT\n# import datetime\n\nimport mne\nfrom mne.utils import _TempDir\n\nimport pd_parser\n\n# get the data\nout_dir = _TempDir()\ncall(['curl -L https://raw.githubusercontent.com/alexrockhill/pd-parser/'\n      'master/pd_parser/tests/data/test_video.mp4 '\n      '-o ' + op.join(out_dir, 'test_video.mp4')], shell=True, env=os.environ)\ncall(['curl -L https://raw.githubusercontent.com/alexrockhill/pd-parser/'\n      'master/pd_parser/tests/data/test_video.wav '\n      '-o ' + op.join(out_dir, 'test_video.wav')], shell=True, env=os.environ)\ncall(['curl -L https://raw.githubusercontent.com/alexrockhill/pd-parser/'\n      'master/pd_parser/tests/data/test_video_beh.tsv '\n      '-o ' + op.join(out_dir, 'test_video_beh.tsv')],\n     shell=True, env=os.environ)\n\n# navigate to the example video\nvideo_fname = op.join(out_dir, 'test_video.mp4')\n\naudio_fname = video_fname.replace('mp4', 'wav')  # pre-computed\n# extract audio (requires ffmpeg)\n# run(['ffmpeg', '-i', video_fname, audio_fname])\n\nfs, data = wavfile.read(audio_fname)\ndata = data.mean(axis=1)  # stereo audio but only need one source\ninfo = mne.create_info(['audio'], fs, ['stim'])\nraw = mne.io.RawArray(data[np.newaxis], info)\n\n# find audio-visual time offset\noffset = 0  # pre-computed value for this video\n'''\nresult = run(['ffprobe', '-show_entries', 'stream=codec_type,start_time',\n              '-v', '0', '-of', 'compact=p=1:nk=0', video_fname],\n             stdout=PIPE, stderr=STDOUT)\noutput = result.stdout.decode('utf-8').split('\\n')\noffset = float(output[0].strip('stream|codec_type=video|start_time')) - \\\n    float(output[1].strip('stream|codec_type=audio|start_time'))\n'''\n\n# save to disk as required by ``pd-parser``, raw needs a filename\nfname = op.join(out_dir, 'sub-1_task-mytask_raw.fif')\nraw.save(fname)\n\n# navigate to corresponding behavior\nbehf = op.join(out_dir, 'test_video_beh.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the parser\n\nNow we'll call the main function to automatically parse the audio events.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "annot, samples = pd_parser.parse_audio(fname, beh=behf,\n                                       beh_key='tone_onset_time',\n                                       audio_ch_names=['audio'], zscore=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the results\n\nFinally, we'll load the events and use them to crop the video although it\nrequires ffmpeg so it is commented out.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Here are the event times: ', annot.onset)\n\n# Crop the videos with ffmpeg\n'''\nfrom pd_parser.parse_pd import _read_tsv\nbeh = _read_tsv(behf)\nfor i in range(annot.onset.size):  # skip the first video\n    action_time = (beh['tone_onset'][i] - beh['action_onset'][i]) / 1000\n    run(['ffmpeg', '-i', f'{video_fname}', '-ss',\n         str(datetime.timedelta(\n             seconds=annot.onset[i] - action_time - offset)),\n         '-to', str(datetime.timedelta(seconds=annot.onset[i] - offset)),\n         op.join(out_dir, 'movement-{}+action_type-{}.mp4'.format(\n             beh['movement'][i], beh['action_type'][i]))])\n'''"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}